{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0929543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: scikit-learn in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: matplotlib in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from matplotlib) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from matplotlib) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from seaborn) (2.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: pandas in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: tqdm in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in g:\\vlsi\\ic_technology\\project\\project-root\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib seaborn\n",
    "!pip install pandas\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70028045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce 940MX\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f60a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69511bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_16976\\3236857814.py:6: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  data = pickle.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['dieSize', 'failureType', 'lotName', 'trainTestLabel', 'waferIndex',\n",
      "       'waferMap'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Path to the dataset file (change if required)\n",
    "DATASET_PATH = r\"G:\\VLSI\\IC_technology\\Project\\project-root\\data\\WM811K.pkl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(DATASET_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Check keys in the loaded data\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77300777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_16976\\3421921335.py:5: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  data = pickle.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(811457, 6)\n",
      "Index(['dieSize', 'failureType', 'lotName', 'trainTestLabel', 'waferIndex',\n",
      "       'waferMap'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (it's a DataFrame)\n",
    "with open(DATASET_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Check the shape and column names\n",
    "print(data.shape)\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ff71f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dieSize</th>\n",
       "      <th>failureType</th>\n",
       "      <th>lotName</th>\n",
       "      <th>trainTestLabel</th>\n",
       "      <th>waferIndex</th>\n",
       "      <th>waferMap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1683.0</td>\n",
       "      <td>none</td>\n",
       "      <td>lot1</td>\n",
       "      <td>Training</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1683.0</td>\n",
       "      <td>none</td>\n",
       "      <td>lot1</td>\n",
       "      <td>Training</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1683.0</td>\n",
       "      <td>none</td>\n",
       "      <td>lot1</td>\n",
       "      <td>Training</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1683.0</td>\n",
       "      <td>none</td>\n",
       "      <td>lot1</td>\n",
       "      <td>Training</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1683.0</td>\n",
       "      <td>none</td>\n",
       "      <td>lot1</td>\n",
       "      <td>Training</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dieSize failureType lotName trainTestLabel  waferIndex  \\\n",
       "0   1683.0        none    lot1       Training         1.0   \n",
       "1   1683.0        none    lot1       Training         2.0   \n",
       "2   1683.0        none    lot1       Training         3.0   \n",
       "3   1683.0        none    lot1       Training         4.0   \n",
       "4   1683.0        none    lot1       Training         5.0   \n",
       "\n",
       "                                            waferMap  \n",
       "0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "356cd14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot 5 wafer maps from your data\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    wafer_map = data.iloc[i]['waferMap']\n",
    "    label = data.iloc[i]['failureType']\n",
    "    plt.imshow(wafer_map, cmap='gray')\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9276968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class WaferDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.labels = dataframe['failureType'].values\n",
    "        self.wafer_maps = dataframe['waferMap'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wafer_map = self.wafer_maps[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert numpy array to PIL Image for torchvision transforms\n",
    "        wafer_img = Image.fromarray(np.uint8(wafer_map * 255))  # Scale to 0-255 if not already\n",
    "\n",
    "        if self.transform:\n",
    "            wafer_img = self.transform(wafer_img)\n",
    "\n",
    "        return wafer_img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fe6f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms (no median filter yet)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # Converts to [0,1] tensor automatically\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7847a51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "\n",
    "# Pick a sample wafer image from your dataframe\n",
    "sample_wafer = data.iloc[0]['waferMap']\n",
    "\n",
    "# Convert numpy array to PIL image\n",
    "wafer_img = Image.fromarray(sample_wafer.astype(np.uint8))\n",
    "\n",
    "# Apply Median filter with size 9\n",
    "wafer_filtered = wafer_img.filter(ImageFilter.MedianFilter(size=9))\n",
    "\n",
    "# Plot original and filtered image\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(wafer_img, cmap='gray')\n",
    "plt.title('Original Wafer Map')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(wafer_filtered, cmap='gray')\n",
    "plt.title('Median Filtered (9x9)')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d66d4faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failureType\n",
      "<class 'numpy.uint64'>    638507\n",
      "<class 'str'>             172950\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Flatten the failureType column: if it's a numpy array, extract its first element\n",
    "def extract_label(x):\n",
    "    if isinstance(x, (np.ndarray, list)):\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# Apply to the whole column\n",
    "data['failureType'] = data['failureType'].apply(extract_label)\n",
    "\n",
    "# Now check the types\n",
    "print(data['failureType'].apply(type).value_counts())  # Should show only <class 'str'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee555e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failureType\n",
      "<class 'str'>    172950\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Keep only rows where failureType is a string (the labeled wafer maps)\n",
    "data = data[data['failureType'].apply(lambda x: isinstance(x, str))].reset_index(drop=True)\n",
    "\n",
    "# Confirm the fix\n",
    "print(data['failureType'].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f69fa517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names: ['Center', 'Donut', 'Edge-Loc', 'Edge-Ring', 'Loc', 'Near-full', 'Random', 'Scratch', 'none']\n",
      "Class to index mapping: {'Center': 0, 'Donut': 1, 'Edge-Loc': 2, 'Edge-Ring': 3, 'Loc': 4, 'Near-full': 5, 'Random': 6, 'Scratch': 7, 'none': 8}\n"
     ]
    }
   ],
   "source": [
    "# Now the unique class names should work correctly\n",
    "class_names = sorted(data['failureType'].unique())\n",
    "print(\"Class names:\", class_names)\n",
    "\n",
    "class_to_idx = {label: idx for idx, label in enumerate(class_names)}\n",
    "print(\"Class to index mapping:\", class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d88d8955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaferDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, apply_filter=True):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.apply_filter = apply_filter\n",
    "        self.wafer_maps = dataframe['waferMap'].values\n",
    "        self.labels = dataframe['failureType'].map(class_to_idx).values  # Convert to numeric labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wafer_map = self.wafer_maps[idx]\n",
    "\n",
    "        # Convert numpy to PIL image\n",
    "        wafer_img = Image.fromarray(np.uint8(wafer_map * 255))  # scale to 0-255\n",
    "\n",
    "        # Apply median filtering\n",
    "        if self.apply_filter:\n",
    "            wafer_img = wafer_img.filter(ImageFilter.MedianFilter(size=9))\n",
    "\n",
    "        # Apply additional transforms like resizing, normalization\n",
    "        if self.transform:\n",
    "            wafer_img = self.transform(wafer_img)\n",
    "\n",
    "        # Get numeric label\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return wafer_img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c83174dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 1, 224, 224])\n",
      "Label batch: tensor([8, 4, 8, 8, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8, 8,\n",
      "        8, 8, 8, 3, 3, 8, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# Split the labeled data into train/test\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, stratify=data['failureType'], random_state=42)\n",
    "\n",
    "# Define image transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = WaferDataset(train_df, transform=transform, apply_filter=True)\n",
    "test_dataset = WaferDataset(test_df, transform=transform, apply_filter=True)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test dataloader output\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Image batch shape: {images.shape}\")\n",
    "    print(f\"Label batch: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "278c9f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary labels batch: tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Function to convert multi-class label to binary (defect: 1, none: 0)\n",
    "def label_to_binary(label):\n",
    "    return 0 if label == class_to_idx['none'] else 1\n",
    "\n",
    "# Create new Datasets with binary labels\n",
    "class BinaryWaferDataset(WaferDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = super().__getitem__(idx)\n",
    "        binary_label = label_to_binary(label)\n",
    "        return image, binary_label\n",
    "\n",
    "# Create train/test binary datasets\n",
    "train_dataset_binary = BinaryWaferDataset(train_df, transform=transform, apply_filter=True)\n",
    "test_dataset_binary = BinaryWaferDataset(test_df, transform=transform, apply_filter=True)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_binary = DataLoader(train_dataset_binary, batch_size=batch_size, shuffle=True)\n",
    "test_loader_binary = DataLoader(test_dataset_binary, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test batch\n",
    "for images, labels in train_loader_binary:\n",
    "    print(f\"Binary labels batch: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00105aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\VLSI\\IC_technology\\Project\\project-root\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "g:\\VLSI\\IC_technology\\Project\\project-root\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained ResNet18\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify first conv layer to accept 1-channel images\n",
    "resnet18.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Modify final fully connected layer for binary classification (2 output classes)\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 2)\n",
    "\n",
    "# Move to GPU if available\n",
    "resnet18 = resnet18.to(device)\n",
    "\n",
    "# Print the model summary\n",
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7e2e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Because we have two classes (0 or 1)\n",
    "optimizer = optim.Adam(resnet18.parameters(), lr=0.001)\n",
    "\n",
    "# Accuracy function\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    predicted = torch.argmax(y_pred, dim=1)\n",
    "    correct = (predicted == y_true).sum().item()\n",
    "    accuracy = correct / y_true.size(0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67bbd2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|| 4324/4324 [1:02:33<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 0.1856, Accuracy: 0.9429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|| 4324/4324 [1:03:31<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Loss: 0.1508, Accuracy: 0.9565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|| 4324/4324 [1:03:24<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Loss: 0.1314, Accuracy: 0.9621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|| 4324/4324 [1:02:45<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Loss: 0.1254, Accuracy: 0.9640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|| 4324/4324 [1:02:44<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Loss: 0.1226, Accuracy: 0.9646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    resnet18.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    for images, labels in tqdm(train_loader_binary, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = resnet18(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = calculate_accuracy(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_acc += acc\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader_binary)\n",
    "    epoch_acc = running_acc / len(train_loader_binary)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6bf6d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Loss: 10.9092, Accuracy: 0.1478\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "resnet18.eval()  # Set model to eval mode\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_binary:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = resnet18(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        acc = calculate_accuracy(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_acc += acc\n",
    "\n",
    "test_loss /= len(test_loader_binary)\n",
    "test_acc /= len(test_loader_binary)\n",
    "\n",
    "print(f\"[Test] Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcfea9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class labels batch: tensor([2, 8, 3, 8, 2, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 3, 8, 8, 8, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# Multi-class train/test datasets (already done earlier)\n",
    "train_dataset_multi = WaferDataset(train_df, transform=transform, apply_filter=True)\n",
    "test_dataset_multi = WaferDataset(test_df, transform=transform, apply_filter=True)\n",
    "\n",
    "# Multi-class DataLoaders\n",
    "train_loader_multi = DataLoader(train_dataset_multi, batch_size=batch_size, shuffle=True)\n",
    "test_loader_multi = DataLoader(test_dataset_multi, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test one batch\n",
    "for images, labels in train_loader_multi:\n",
    "    print(f\"Multi-class labels batch: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "933118c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\VLSI\\IC_technology\\Project\\project-root\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "g:\\VLSI\\IC_technology\\Project\\project-root\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Reload pre-trained ResNet18\n",
    "resnet18_multi = models.resnet18(pretrained=True)\n",
    "\n",
    "# Change input conv1 to 1-channel\n",
    "resnet18_multi.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Modify final fully connected layer to 9 output classes\n",
    "resnet18_multi.fc = nn.Linear(resnet18_multi.fc.in_features, 9)\n",
    "\n",
    "# Move to device\n",
    "resnet18_multi = resnet18_multi.to(device)\n",
    "\n",
    "# Print summary\n",
    "print(resnet18_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "893d5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for multi-class\n",
    "criterion_multi = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer_multi = optim.Adam(resnet18_multi.parameters(), lr=0.001)\n",
    "\n",
    "# Accuracy function (same as before)\n",
    "def calculate_accuracy_multi(y_pred, y_true):\n",
    "    predicted = torch.argmax(y_pred, dim=1)\n",
    "    correct = (predicted == y_true).sum().item()\n",
    "    accuracy = correct / y_true.size(0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e424bec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|| 4324/4324 [1:04:37<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 0.2987, Accuracy: 0.9256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|| 4324/4324 [1:02:48<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Loss: 0.2363, Accuracy: 0.9400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|| 4324/4324 [1:03:13<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Loss: 0.2134, Accuracy: 0.9452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|| 4324/4324 [1:01:52<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Loss: 0.2004, Accuracy: 0.9482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|| 4324/4324 [1:01:36<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Loss: 0.1915, Accuracy: 0.9507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    resnet18_multi.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    for images, labels in tqdm(train_loader_multi, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer_multi.zero_grad()\n",
    "\n",
    "        outputs = resnet18_multi(images)\n",
    "        loss = criterion_multi(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_multi.step()\n",
    "\n",
    "        acc = calculate_accuracy_multi(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_acc += acc\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader_multi)\n",
    "    epoch_acc = running_acc / len(train_loader_multi)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "882e40c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Loss: 0.5203, Accuracy: 0.8960\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "resnet18_multi.eval()\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_multi:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = resnet18_multi(images)\n",
    "        loss = criterion_multi(outputs, labels)\n",
    "\n",
    "        acc = calculate_accuracy_multi(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_acc += acc\n",
    "\n",
    "test_loss /= len(test_loader_multi)\n",
    "test_acc /= len(test_loader_multi)\n",
    "\n",
    "print(f\"[Test] Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f20351e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetFeatureExtractor(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create a feature extractor from trained resnet18_multi\n",
    "from torch import nn\n",
    "\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ResNetFeatureExtractor, self).__init__()\n",
    "        # Everything except the final fc layer\n",
    "        self.features = nn.Sequential(\n",
    "            model.conv1,\n",
    "            model.bn1,\n",
    "            model.relu,\n",
    "            model.maxpool,\n",
    "            model.layer1,\n",
    "            model.layer2,\n",
    "            model.layer3,\n",
    "            model.layer4,\n",
    "            model.avgpool,  # outputs 512-dim (1, 512, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x.view(x.size(0), -1)  # flatten (B, 512)\n",
    "\n",
    "# Instantiate and freeze\n",
    "feature_extractor = ResNetFeatureExtractor(resnet18_multi).to(device)\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b35502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  40%|      | 1716/4324 [09:07<12:13,  3.55it/s] "
     ]
    }
   ],
   "source": [
    "#  Safer version (writes to disk, no RAM crash)\n",
    "import os\n",
    "\n",
    "features_list = []\n",
    "labels_list = []\n",
    "image_paths = []\n",
    "save_every = 1000\n",
    "counter = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(train_loader_multi, desc=\"Extracting & Saving Features\")):\n",
    "        images = images.to(device)\n",
    "        features = feature_extractor(images).cpu().numpy()\n",
    "\n",
    "        features_list.append(features)\n",
    "        labels_list.append(labels.numpy())\n",
    "        image_paths.append(images.cpu().numpy())\n",
    "\n",
    "        counter += len(images)\n",
    "\n",
    "        if counter >= save_every:\n",
    "            np.savez_compressed(f\"features_chunk_{batch_idx}.npz\",\n",
    "                                features=np.concatenate(features_list, axis=0),\n",
    "                                labels=np.concatenate(labels_list, axis=0),\n",
    "                                images=np.concatenate(image_paths, axis=0))\n",
    "            features_list, labels_list, image_paths = [], [], []\n",
    "            counter = 0\n",
    "\n",
    "# Final chunk save\n",
    "if features_list:\n",
    "    np.savez_compressed(f\"features_chunk_final.npz\",\n",
    "                        features=np.concatenate(features_list, axis=0),\n",
    "                        labels=np.concatenate(labels_list, axis=0),\n",
    "                        images=np.concatenate(image_paths, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da2823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "X_features, Y_labels, X_images = [], [], []\n",
    "\n",
    "for file in sorted(glob.glob(\"features_chunk_*.npz\")):\n",
    "    data = np.load(file)\n",
    "    X_features.append(data[\"features\"])\n",
    "    Y_labels.append(data[\"labels\"])\n",
    "    X_images.append(data[\"images\"])\n",
    "\n",
    "# Final combined arrays\n",
    "X_features = np.concatenate(X_features, axis=0)\n",
    "Y_labels = np.concatenate(Y_labels, axis=0)\n",
    "X_images = np.concatenate(X_images, axis=0)\n",
    "\n",
    "print(\"Loaded features shape:\", X_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a596987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Step 1: PCA to reduce to 256 dims\n",
    "pca = PCA(n_components=256)\n",
    "X_pca = pca.fit_transform(X_features)\n",
    "\n",
    "# Step 2: Normalize features to [0, 1] for similarity search\n",
    "scaler = MinMaxScaler()\n",
    "X_pca_norm = scaler.fit_transform(X_pca)\n",
    "\n",
    "print(\"Reduced feature shape:\", X_pca_norm.shape)  # Should be (N, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2793ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def retrieve_similar_wafer(query_img_tensor, top_k=5):\n",
    "    # Preprocess & extract embedding\n",
    "    feature_extractor.eval()\n",
    "    with torch.no_grad():\n",
    "        query_img_tensor = query_img_tensor.unsqueeze(0).to(device)  # Add batch dim\n",
    "        embedding = feature_extractor(query_img_tensor).cpu().numpy()  # (1, 512)\n",
    "\n",
    "    # Apply PCA + normalization\n",
    "    query_pca = pca.transform(embedding)\n",
    "    query_norm = scaler.transform(query_pca)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(query_norm, X_pca_norm)[0]  # shape: (N,)\n",
    "\n",
    "    # Get top K indices\n",
    "    top_k_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    return top_k_indices, similarities[top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984bc453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_similar_wafers(query_img, top_indices, scores):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "\n",
    "    # Plot query\n",
    "    plt.subplot(1, len(top_indices)+1, 1)\n",
    "    plt.imshow(query_img.squeeze(), cmap='gray')\n",
    "    plt.title(\"Query Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Plot top-k retrieved\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        plt.subplot(1, len(top_indices)+1, i+2)\n",
    "        plt.imshow(X_images[idx].squeeze(), cmap='gray')\n",
    "        plt.title(f\"Sim #{i+1} (Score: {scores[i]:.2f})\\nClass: {class_names[Y_labels[idx]]}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b136ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random test wafer image\n",
    "query_img_tensor, true_label = test_dataset_multi[5]  # Change index if needed\n",
    "\n",
    "# Retrieve similar wafers\n",
    "top_k_idx, top_k_scores = retrieve_similar_wafer(query_img_tensor)\n",
    "\n",
    "# Show results\n",
    "show_similar_wafers(query_img_tensor, top_k_idx, top_k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62468b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
